import streamlit as st
import requests
import os
from datetime import datetime
from urllib.parse import quote
import json

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã ---
st.set_page_config(page_title="–ù–æ–≤–æ—Å—Ç–∏ –í—Å–µ–ª–µ–Ω–Ω–æ–π Disney", layout="wide")

st.markdown("""
<style>
    .stApp { background-color: #f0f2f6; }
    body, p, .st-emotion-cache-16txtl3, .st-emotion-cache-1629p8f p, .st-emotion-cache-1xarl3l, h1, h2, h3, h4, h5, h6 {
        color: #111111 !important;
    }
    .st-emotion-cache-16txtl3 { padding-top: 2rem; }
</style>
""", unsafe_allow_html=True)

@st.cache_data(ttl=3600)
def fetch_google_news(search_query):
    """–ò—â–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ Google News API –æ—Ç Serper.dev."""
    api_key = os.getenv("SERPER_API_KEY")
    if not api_key:
        return None, "–ö–ª—é—á SERPER_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Å–µ–∫—Ä–µ—Ç–∞—Ö."

    url = "https://google.serper.dev/news"
    payload = json.dumps({"q": search_query, "gl": "ru", "hl": "ru"})
    headers = {'X-API-KEY': api_key, 'Content-Type': 'application/json'}

    try:
        response = requests.post(url, headers=headers, data=payload)
        if response.status_code == 200:
            results = response.json().get("news", [])
            return results, None
        else:
            return None, f"–û—à–∏–±–∫–∞ API Serper. –°—Ç–∞—Ç—É—Å: {response.status_code}, –û—Ç–≤–µ—Ç: {response.text}"
    except Exception as e:
        return None, f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}"

# === –ò–ù–¢–ï–†–§–ï–ô–° –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø ===
st.title("üåê –î–∞–π–¥–∂–µ—Å—Ç –ù–æ–≤–æ—Å—Ç–µ–π –í—Å–µ–ª–µ–Ω–Ω–æ–π Disney")
st.write("–ü–æ–∏—Å–∫ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ –ª—é–±—ã—Ö —Å–∞–π—Ç–∞—Ö, –≤–∫–ª—é—á–∞—è –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –±–ª–æ–≥–∏.")
st.divider()

# --- –†–∞–∑–¥–µ–ª "–ü–æ—Å–ª–µ–¥–Ω–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏" ---
st.header("üî• –ü–æ—Å–ª–µ–¥–Ω–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏")

relevant_keywords = (
    'Disney OR Pixar OR Marvel OR Lucasfilm OR "Star Wars" OR –î–∏—Å–Ω–µ–π–ª–µ–Ω–¥ '
    'site:thewaltdisneycompany.com OR site:daily.afisha.ru'
)

with st.spinner("–ó–∞–≥—Ä—É–∂–∞—é —Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ Google..."):
    latest_articles, error = fetch_google_news(relevant_keywords)

    if latest_articles:
        st.success(f"–ù–∞–π–¥–µ–Ω–æ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(latest_articles)}")

    if error:
        st.error(error)
    elif latest_articles:
        for article in latest_articles[:7]:
            st.subheader(article['title'])
            date_published_str = article.get('date', '–î–∞—Ç–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞')
            st.caption(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {article['source']} | –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {date_published_str}")
            st.write(article.get('snippet', '–û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.')) # –ò—Å–ø–æ–ª—å–∑—É–µ–º snippet 
            st.markdown(f"[*–ß–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ...*]({article['link']})")
            st.divider()
    else:
        st.info("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Ç–µ–º–∞–º Disney –≤ Google.")

# --- –†–∞–∑–¥–µ–ª "–ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π" ---
st.header("üîç –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤ Google News")
search_term = st.text_input("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'Avatar 4' –∏–ª–∏ 'Bob Iger'):", "Toy Story 5")

if st.button("–ù–∞–π—Ç–∏"):
    if not search_term:
        st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞.")
    else:
        with st.spinner(f"–ò—â—É –≤ Google News –ø–æ –∑–∞–ø—Ä–æ—Å—É '{search_term}'..."):
            articles, error = fetch_google_news(search_term)

            if articles:
                st.success(f"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(articles)}")

            if error:
                st.error(error)
            elif not articles:
                st.info(f"–ù–æ–≤–æ—Å—Ç–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É '{search_term}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
            else:
                for article in articles[:10]:
                    st.subheader(article['title'])
                    date_published_str = article.get('date', '–î–∞—Ç–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞')
                    st.caption(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {article['source']} | –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {date_published_str}")
                    st.write(article.get('snippet', '–û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.'))
                    st.markdown(f"[*–ß–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ...*]({article['link']})")
                    st.divider()
