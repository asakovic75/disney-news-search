import streamlit as st
import requests
import os
from datetime import datetime
from urllib.parse import quote
import json

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã ---
st.set_page_config(page_title="–ù–æ–≤–æ—Å—Ç–∏ –í—Å–µ–ª–µ–Ω–Ω–æ–π Disney", layout="wide")

st.markdown("""
<style>
    .stApp { background-color: #f0f2f6; }
    body, p, .st-emotion-cache-16txtl3, .st-emotion-cache-1629p8f p, .st-emotion-cache-1xarl3l, h1, h2, h3, h4, h5, h6 {
        color: #111111 !important;
    }
    .st-emotion-cache-16txtl3 { padding-top: 2rem; }
</style>
""", unsafe_allow_html=True)

# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Google (Serper.dev) ---
@st.cache_data(ttl=1800) # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ 30 –º–∏–Ω—É—Ç
def fetch_google_news(search_query):
    """–ò—â–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ Google News API –æ—Ç Serper.dev."""
    api_key = os.getenv("SERPER_API_KEY")
    if not api_key:
        return None, "–ö–ª—é—á SERPER_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Å–µ–∫—Ä–µ—Ç–∞—Ö."

    url = "https://google.serper.dev/news"
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∑–∞–ø—Ä–æ—Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∏—Å–∫–∞—Ç—å —Ç–æ–ª—å–∫–æ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ–¥–µ–ª—é –¥–ª—è —Å–≤–µ–∂–µ—Å—Ç–∏
    payload = json.dumps({"q": search_query, "gl": "ru", "hl": "ru", "tbs": "qdr:w"})
    headers = {'X-API-KEY': api_key, 'Content-Type': 'application/json'}

    try:
        response = requests.post(url, headers=headers, data=payload)
        if response.status_code == 200:
            results = response.json().get("news", [])
            return results, None
        else:
            return None, f"–û—à–∏–±–∫–∞ API Serper. –°—Ç–∞—Ç—É—Å: {response.status_code}, –û—Ç–≤–µ—Ç: {response.text}"
    except Exception as e:
        return None, f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}"

# === –ò–ù–¢–ï–†–§–ï–ô–° –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø ===
st.title("üåê –î–∞–π–¥–∂–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –≤—Å–µ–ª–µ–Ω–Ω–æ–π Disney")
st.write("–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å–∞–º—ã—Ö –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –æ –∫–æ–º–ø–∞–Ω–∏—è—Ö, –ø—Ä–æ–µ–∫—Ç–∞—Ö –∏ –ø–∞—Ä–∫–∞—Ö Disney.")
st.divider()

# --- –†–∞–∑–¥–µ–ª "–ü–æ—Å–ª–µ–¥–Ω–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏" ---
st.header("üî• –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏")

# –ö–∞–∂–¥–∞—è —Ñ—Ä–∞–∑–∞ –≤ –∫–∞–≤—ã—á–∫–∞—Ö –∏—â–µ—Ç—Å—è –∫–∞–∫ –µ–¥–∏–Ω–æ–µ —Ü–µ–ª–æ–µ. –û–ø–µ—Ä–∞—Ç–æ—Ä OR –∏—â–µ—Ç —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ.
relevant_keywords = (
    # –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    '"The Walt Disney Company" OR TWDC OR "Bob Iger" OR "Disney earnings" OR '
    # –°—Ç—É–¥–∏–∏ –∏ –±—Ä–µ–Ω–¥—ã
    'Pixar OR "Marvel Studios" OR MCU OR Lucasfilm OR "Star Wars" OR '
    '"20th Century Studios" OR "Searchlight Pictures" OR "Walt Disney Animation Studios" OR '
    # –ü–∞—Ä–∫–∏ –∏ –ø—Ä–æ–¥—É–∫—Ç—ã
    'Disneyland OR "Walt Disney World" OR "Disney Cruise Line" OR "Disney merchandise" OR '
    # –ü–ª–∞—Ç—Ñ–æ—Ä–º—ã
    '"Disney+" OR "Disney Plus" OR Hulu OR '
    # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã (–¥–æ–±–∞–≤–ª–µ–Ω—ã —Ä—É—Å—Å–∫–∏–µ –∞–Ω–∞–ª–æ–≥–∏)
    '"Inside Out" OR "–ì–æ–ª–æ–≤–æ–ª–æ–º–∫–∞" OR "The Mandalorian & Grogu" OR "–ú–∞–Ω–¥–∞–ª–æ—Ä–µ—Ü" OR '
    '"Moana" OR "–ú–æ–∞–Ω–∞" OR "Zootopia" OR "–ó–≤–µ—Ä–æ–ø–æ–ª–∏—Å" OR "Frozen" OR "–•–æ–ª–æ–¥–Ω–æ–µ —Å–µ—Ä–¥—Ü–µ" OR '
    '"Toy Story 5" OR "Snow White" OR "Avatar"'
)

with st.spinner("–ó–∞–≥—Ä—É–∂–∞—é —Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ Google –∑–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ–¥–µ–ª—é..."):
    latest_articles, error = fetch_google_news(relevant_keywords)

    if error:
        st.error(error)
    elif latest_articles:
        st.success(f"–ù–∞–π–¥–µ–Ω–æ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –≤–∞—à–∏–º –∫–ª—é—á–µ–≤—ã–º —Ç–µ–º–∞–º: {len(latest_articles)}")
        for article in latest_articles[:10]: # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ 10 –Ω–æ–≤–æ—Å—Ç–µ–π
            st.subheader(article['title'])
            date_published_str = article.get('date', '–î–∞—Ç–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞')
            st.caption(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {article['source']} | –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {date_published_str}")
            st.write(article.get('snippet', '–û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.'))
            st.markdown(f"[*–ß–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ...*]({article['link']})")
            st.divider()
    else:
        st.info("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –≤–∞—à–∏–º –∫–ª—é—á–µ–≤—ã–º —Ç–µ–º–∞–º –∑–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ–¥–µ–ª—é.")

# --- –†–∞–∑–¥–µ–ª "–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫" ---
st.header("üîç –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫")
st.write("–ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏–∑ –≤–∞—à–µ–≥–æ —Å–ø–∏—Å–∫–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å –∏–º–µ–Ω–∞–º–∏ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏–ª–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏.")

# –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
st.info('–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤: `Bob Chapek`, ` Zootopia`, ` Toy Story 5`')

search_term = st.text_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à —Ç–æ—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞:", "")

if st.button("–ù–∞–π—Ç–∏"):
    if not search_term:
        st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞.")
    else:
        with st.spinner(f"–ò—â—É –≤ Google News –ø–æ –∑–∞–ø—Ä–æ—Å—É '{search_term}'..."):
            articles, error = fetch_google_news(search_term)

            if error:
                st.error(error)
            elif not articles:
                st.info(f"–ù–æ–≤–æ—Å—Ç–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É '{search_term}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
            else:
                st.success(f"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(articles)}")
                for article in articles[:15]:
                    st.subheader(article['title'])
                    date_published_str = article.get('date', '–î–∞—Ç–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞')
                    st.caption(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {article['source']} | –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {date_published_str}")
                    st.write(article.get('snippet', '–û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.'))
                    st.markdown(f"[*–ß–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ...*]({article['link']})")
                    st.divider()


